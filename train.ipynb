{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'woman']\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = r\"C:\\Users\\user\\Desktop\\Gender Detection\\dataset\"\n",
    "print(os.listdir(DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['man', 'woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size = (224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "\trotation_range=20,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=baseModel.input, outputs=headModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 81s 1s/step - loss: 0.6479 - accuracy: 0.6784 - val_loss: 0.4180 - val_accuracy: 0.8139\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.4029 - accuracy: 0.8103 - val_loss: 0.3250 - val_accuracy: 0.8701\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.3911 - accuracy: 0.8191 - val_loss: 0.3217 - val_accuracy: 0.8701\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.3275 - accuracy: 0.8582 - val_loss: 0.2968 - val_accuracy: 0.8810\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.3023 - accuracy: 0.8660 - val_loss: 0.2768 - val_accuracy: 0.8939\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 79s 1s/step - loss: 0.3051 - accuracy: 0.8671 - val_loss: 0.2800 - val_accuracy: 0.8918\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 79s 1s/step - loss: 0.2851 - accuracy: 0.8665 - val_loss: 0.2750 - val_accuracy: 0.8983\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 79s 1s/step - loss: 0.2778 - accuracy: 0.8764 - val_loss: 0.2611 - val_accuracy: 0.9004\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.2595 - accuracy: 0.8908 - val_loss: 0.2574 - val_accuracy: 0.9004\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 80s 1s/step - loss: 0.2796 - accuracy: 0.8825 - val_loss: 0.2473 - val_accuracy: 0.8939\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 79s 1s/step - loss: 0.2482 - accuracy: 0.8941 - val_loss: 0.2468 - val_accuracy: 0.8939\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.2492 - accuracy: 0.8941 - val_loss: 0.2523 - val_accuracy: 0.8961\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 78s 1s/step - loss: 0.2445 - accuracy: 0.8924 - val_loss: 0.2500 - val_accuracy: 0.8939\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 69s 1s/step - loss: 0.2411 - accuracy: 0.9013 - val_loss: 0.2447 - val_accuracy: 0.8939\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 58s 1s/step - loss: 0.2397 - accuracy: 0.8969 - val_loss: 0.2321 - val_accuracy: 0.9091\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 58s 1s/step - loss: 0.2459 - accuracy: 0.8930 - val_loss: 0.2208 - val_accuracy: 0.9134\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 58s 1s/step - loss: 0.2268 - accuracy: 0.9095 - val_loss: 0.2467 - val_accuracy: 0.8983\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 60s 1s/step - loss: 0.2329 - accuracy: 0.9013 - val_loss: 0.2333 - val_accuracy: 0.9004\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 59s 1s/step - loss: 0.2151 - accuracy: 0.9129 - val_loss: 0.2482 - val_accuracy: 0.8983\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 59s 1s/step - loss: 0.2198 - accuracy: 0.9046 - val_loss: 0.2365 - val_accuracy: 0.9026\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tvalidation_steps=len(testX) // BS,\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"pic.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (224, 224))\n",
    "image = np.array([image])\n",
    "prediction = model.predict(image)\n",
    "predIdxs = np.argmax(prediction, axis=1)\n",
    "print(predIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predIdxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n"
     ]
    }
   ],
   "source": [
    "print(CATEGORIES[predIdxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
